{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [Root]","language":"python","name":"Python [Root]"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"},"colab":{"name":"Web scraping.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"3YiUyNj75OtN","colab_type":"text"},"source":["<h2>Caveat</h2>\n","Web sites often change the format of their pages so this may not always work. If it doesn't, rework the examples after examining the html content of the page (most browsers will let you see the html source - look for a \"page source\" option - though you might have to turn on the developer mode in your browser preferences. For example, on Chrome you need to click the \"developer mode\" check box under Extensions in the Preferences/Options menu. "]},{"cell_type":"markdown","metadata":{"id":"zKNA8Aby5OtO","colab_type":"text"},"source":["<h1>Scraping web pages</h1>\n","<li>Most data that resides on the web is in HTML \n","<li>HTML: HyperText Markup Language\n","<li>An html web page is a structured document\n","<li>We can exploit this structure to extract data from the page"]},{"cell_type":"markdown","metadata":{"id":"qT3B_3vJ5OtO","colab_type":"text"},"source":["<li>Learn html and css at <a href=\"https://www.khanacademy.org/computing/computer-programming/html-css\">this site</a>"]},{"cell_type":"markdown","metadata":{"id":"X0cUfByn5OtP","colab_type":"text"},"source":["<b>Web scraping</b>: Automating the process of extracting information from web pages<br>\n","<li>for data collection and analysis\n","<li>for incorporating in a web app "]},{"cell_type":"markdown","metadata":{"id":"Zbt1Kj7a5OtP","colab_type":"text"},"source":["<h2>Python libraries for web scraping</h2>\n","<li><b>requests</b> for handling the request-response cycle\n","<li><b>beautifulsoup4</b> for extracting data from an html string\n","<li><b>selenium</b> for extracting data from an html string and managing the response process, particularly when a page contains JavaScript or when a button needs to be clicked"]},{"cell_type":"markdown","metadata":{"id":"1ysn3rH85OtQ","colab_type":"text"},"source":["<h2>Beautiful Soup</h2>\n","<li>html and xml parser\n","<li>makes use of formatted html tags and css properties to extract data\n","<li>https://www.crummy.com/software/BeautifulSoup/bs4/doc/"]},{"cell_type":"markdown","metadata":{"id":"Y-LScbHV5OtQ","colab_type":"text"},"source":["<h2>Web scraping using beautifulsoup4</h2>"]},{"cell_type":"markdown","metadata":{"id":"dRyKNWPu5OtR","colab_type":"text"},"source":["<h3>Import necessary modules</h3>"]},{"cell_type":"code","metadata":{"id":"F62wVQxr5OtR","colab_type":"code","colab":{}},"source":["import requests\n","from bs4 import BeautifulSoup"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rYdzn7Ag5OtT","colab_type":"text"},"source":["<h3>The http request response cycle</h3>"]},{"cell_type":"code","metadata":{"id":"DNXWXOsp5OtU","colab_type":"code","colab":{}},"source":["url = \"http://www.epicurious.com/search/Tofu Chili\"\n","response = requests.get(url)\n","if response.status_code == 200:\n","    print(\"Success\")\n","else:\n","    print(\"Failure\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pA6Xkdda5OtW","colab_type":"code","colab":{}},"source":["keywords = input(\"Please enter the things you want to see in a recipe\")\n","url = \"http://www.epicurious.com/search/\" + keywords\n","response = requests.get(url)\n","if response.status_code == 200:\n","    print(\"Success\")\n","else:\n","    print(\"Failure\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EUzqWfTH5OtX","colab_type":"text"},"source":["<h3>Set up the BeautifulSoup object</h3>"]},{"cell_type":"code","metadata":{"id":"piIt3RVK5OtY","colab_type":"code","colab":{}},"source":["results_page = BeautifulSoup(response.content,'lxml')\n","print(results_page.prettify())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lueb1QEk5OtZ","colab_type":"text"},"source":["<h3>BS4 functions</h3>"]},{"cell_type":"markdown","metadata":{"id":"Hy4zHFnB5Ota","colab_type":"text"},"source":["<h4>find_all finds all instances of a specified tag</h4>\n","<h4>returns a result_set (a list)</h4>"]},{"cell_type":"code","metadata":{"id":"JHW3KEBN5Ota","colab_type":"code","colab":{}},"source":["all_a_tags = results_page.find_all('a')\n","print(type(all_a_tags))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3bS7uGXy5Otc","colab_type":"code","colab":{}},"source":["all_a_tags"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bme5fQTs5Ote","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SwCbZk1m5Otg","colab_type":"text"},"source":["<h4>find finds the first instance of a specified tag</h4>\n","<h4>returns a bs4 element</h4>\n"]},{"cell_type":"code","metadata":{"id":"K-vkOY4H5Otg","colab_type":"code","colab":{}},"source":["div_tag = results_page.find('div')\n","print(div_tag)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yc74BDVG5Oth","colab_type":"code","colab":{}},"source":["type(div_tag)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0JUlBpX5Otj","colab_type":"text"},"source":["<h4>bs4 functions can be recursively applied on elements</h4>"]},{"cell_type":"code","metadata":{"id":"FN6ZTCtd5Otj","colab_type":"code","colab":{}},"source":["div_tag.find('a')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jq0ZJX-G5Otl","colab_type":"text"},"source":["<h4>Both find as well as find_all can be qualified by css selectors</h4>\n","<li>using selector=value\n","<li>using a dictionary"]},{"cell_type":"markdown","metadata":{"id":"LAYKzZHn5Otl","colab_type":"text"},"source":["<h4>Using selector=value</h4>"]},{"cell_type":"code","metadata":{"id":"Al-xtxM75Otm","colab_type":"code","colab":{}},"source":["#When using this method and looking for 'class' use 'class_' (because class is a reserved word in python)\n","#Note that we get a list back because find_all returns a list\n","results_page.find_all('article',class_=\"recipe-content-card\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5_b6H3Cv5Otn","colab_type":"text"},"source":["<h4>Using selectors as key value pairs in a dictionary</h4>"]},{"cell_type":"code","metadata":{"id":"TGNqcZFC5Oto","colab_type":"code","colab":{}},"source":["#Since we're using a string as the key, the fact that class is a reserved word is not a problem\n","#We get an element back because find returns an element\n","results_page.find('article',{'class':'recipe-content-card'})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_ub8-FN5Otp","colab_type":"text"},"source":["<h4>get_text() returns the marked up text (the content) enclosed in a tag.</h4>\n","<li>returns a string"]},{"cell_type":"code","metadata":{"id":"AUkYQYcX5Otp","colab_type":"code","colab":{}},"source":["results_page.find('article',{'class':'recipe-content-card'}).get_text()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gaq8_2bq5Otr","colab_type":"text"},"source":["<h4>get returns the value of a tag attribute</h4>\n","<li>returns a string"]},{"cell_type":"code","metadata":{"id":"LxX3CoX45Otr","colab_type":"code","colab":{}},"source":["recipe_tag = results_page.find('article',{'class':'recipe-content-card'})\n","recipe_link = recipe_tag.find('a')\n","print(\"a tag:\",recipe_link)\n","link_url = recipe_link.get('href')\n","print(\"link url:\",link_url)\n","print(type(link_url))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TBj12OVy5Ott","colab_type":"text"},"source":["<h2>Summary of bs4 functions</h2>\n","\n"," ![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"D3mhGO7k5Ott","colab_type":"text"},"source":["<h1>A function that returns a list containing recipe names, recipe descriptions (if any) and recipe urls</h1>"]},{"cell_type":"markdown","metadata":{"id":"BIlVJ6fD5Otv","colab_type":"text"},"source":["\n","<li>We want to create a list of recipes and links to the recipes\n","<li>We need to figure out how to ‘programmatically’ extract each recipe name and recipe link\n","\n","<li>Search for the tag with a unique attribute value that identifies recipes and recipe links\n","<li>We’ll look at the a (annotate) tags because clickable links are in a tags"]},{"cell_type":"code","metadata":{"id":"zprFPyFc5Otw","colab_type":"code","colab":{}},"source":["for tag in results_page.find_all('article'):\n","    print(tag)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VuBFMk2E5Otx","colab_type":"code","colab":{}},"source":["def get_recipes(keywords):\n","    recipe_list = list()\n","    import requests\n","    from bs4 import BeautifulSoup\n","    url = \"http://www.epicurious.com/search/\" + keywords\n","    response = requests.get(url)\n","    if not response.status_code == 200:\n","        return None\n","    try:\n","        results_page = BeautifulSoup(response.content,'lxml')\n","        recipes = results_page.find_all('article',class_=\"recipe-content-card\")\n","        for recipe in recipes:\n","            recipe_link = \"http://www.epicurious.com\" + recipe.find('a').get('href')\n","            recipe_name = recipe.find('a').get_text()\n","            try:\n","                recipe_description = recipe.find('p',class_='dek').get_text()\n","            except:\n","                recipe_description = ''\n","            recipe_list.append((recipe_name,recipe_link,recipe_description))\n","        return recipe_list\n","    except:\n","        return None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3LMWfZ65Otz","colab_type":"code","colab":{}},"source":["get_recipes(\"Tofu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANTaOZCK5Ot1","colab_type":"code","colab":{}},"source":["get_recipes('Nothing')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tH2EwzkS5Ot3","colab_type":"text"},"source":["<h2>Let's write a function that</h2>\n","<h3>given a recipe link</h3>\n","<h3>returns a dictionary containing the ingredients and preparation instructions</h3>"]},{"cell_type":"code","metadata":{"id":"HsQRbv5K5Ot4","colab_type":"code","colab":{}},"source":["recipe_link = \"http://www.epicurious.com\" + '/recipes/food/views/spicy-lemongrass-tofu-233844'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pTi7asyN5Ot5","colab_type":"code","colab":{}},"source":["def get_recipe_info(recipe_link):\n","    recipe_dict = dict()\n","    import requests\n","    from bs4 import BeautifulSoup\n","    try:\n","        response = requests.get(recipe_link)\n","        if not response.status_code == 200:\n","            return recipe_dict\n","        result_page = BeautifulSoup(response.content,'lxml')\n","        ingredient_list = list()\n","        prep_steps_list = list()\n","        for ingredient in result_page.find_all('li',class_='ingredient'):\n","            ingredient_list.append(ingredient.get_text())\n","        for prep_step in result_page.find_all('li',class_='preparation-step'):\n","            prep_steps_list.append(prep_step.get_text().strip())\n","        recipe_dict['ingredients'] = ingredient_list\n","        recipe_dict['preparation'] = prep_steps_list\n","        return recipe_dict\n","    except:\n","        return recipe_dict\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SKwNjYVC5Ot7","colab_type":"code","colab":{}},"source":["get_recipe_info(recipe_link)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iDqyiaOf5Ot-","colab_type":"text"},"source":["<h2>Construct a list of dictionaries for all recipes</h2>"]},{"cell_type":"code","metadata":{"id":"o70Lir2D5Ot_","colab_type":"code","colab":{}},"source":["def get_all_recipes(keywords):\n","    results = list()\n","    all_recipes = get_recipes(keywords)\n","    for recipe in all_recipes:\n","        recipe_dict = get_recipe_info(recipe[1])\n","        recipe_dict['name'] = recipe[0]\n","        recipe_dict['description'] = recipe[2]\n","        results.append(recipe_dict)\n","    return(results)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WsnRXyTA5OuA","colab_type":"code","colab":{}},"source":["get_all_recipes(\"Tofu\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAZ5YZMi5OuC","colab_type":"text"},"source":["<h1>Logging in to a web server</h1>\n","\n","<li>Figure out the login url \n","<li>https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Main+Page\n","<li>Look for the login form in the html source\n","<li>form_tag = page_soup.find('form')\n","<li>Look for ALL the inputs in the login form (some may be tricky!)\n","<li>input_tags = form_tag.find_all('input')\n","<li>Create a Python dict object with key,value pairs for each input\n","<li>Use requests.session to create an open session object\n","<li>Send the login request (POST)\n","<li>Send followup requests keeping the sessions object open"]},{"cell_type":"markdown","metadata":{"id":"09IXo9bo5OuD","colab_type":"text"},"source":["<h2>Get username and password</h2>\n","<li>Best to store in a file for reuse\n","<li>You will need to set up your own login and password and place them in a file called wikidata.txt\n","<li>Line one of the file should contain your username\n","<li>Line two your password"]},{"cell_type":"code","metadata":{"id":"wjbCM15C5OuD","colab_type":"code","colab":{}},"source":["with open('/Users/hardeepjohar/Documents/Courses/Fall2018/API_KEYS/wikidata.txt') as f:\n","    contents = f.read().split('\\n')\n","    username = contents[0]\n","    password = contents[1]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kNl1ULJi5OuE","colab_type":"text"},"source":["<h3>Construct an object that contains the data to be sent to the login page</h3>"]},{"cell_type":"code","metadata":{"id":"4gsJTCq95OuF","colab_type":"code","colab":{}},"source":["\n","payload = {\n","    'wpName': username,\n","    'wpPassword': password,\n","    'wploginattempt': 'Log in',\n","    'wpEditToken': \"+\\\\\",\n","    'title': \"Special:UserLogin\",\n","    'authAction': \"login\",\n","    'force': \"\",\n","    'wpForceHttps': \"1\",\n","    'wpFromhttp': \"1\",\n","    #'wpLoginToken': ‘', #We need to read this from the page\n","    }"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8cLciYLl5OuG","colab_type":"text"},"source":["<h3>get the value of the login token</h3>"]},{"cell_type":"code","metadata":{"id":"4cDub-3o5OuG","colab_type":"code","colab":{}},"source":["def get_login_token(response):\n","    soup = BeautifulSoup(response.text, 'lxml')\n","    token = soup.find('input',{'name':\"wpLoginToken\"}).get('value')\n","    return token\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W4vNik1b5OuI","colab_type":"text"},"source":["<h3>Setup a session, login, and get data</h3>"]},{"cell_type":"code","metadata":{"id":"rElAwzg75OuI","colab_type":"code","colab":{}},"source":["import requests\n","from bs4 import BeautifulSoup\n","with requests.session() as s:\n","    response = s.get('https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Main+Page')\n","    payload['wpLoginToken'] = get_login_token(response)\n","    #Send the login request\n","    response_post = s.post('https://en.wikipedia.org/w/index.php?title=Special:UserLogin&action=submitlogin&type=login',\n","                           data=payload)\n","    #Get another page and check if we’re still logged in\n","    response = s.get('https://en.wikipedia.org/wiki/Special:Watchlist')\n","    data = BeautifulSoup(response.content,'lxml')\n","    print(data.find('div',class_='mw-changeslist').get_text())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OPdAvN-u5OuK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}